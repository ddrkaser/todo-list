MRMC has identified several key issues:

1. Clarification of Model Outputs: The presentation lacks clear definition regarding the model outputs. It would be beneficial to elaborate further on this aspect. For instance, specifying that the model output consists of a numeric score ranging from 0 to 100, where 100 signifies a high likelihood of success for the Financial Advisor (FA) at UBS, and 0 indicates the least likelihood. Additionally, detailing the process of ranking FAs based on their model score and assigning star ratings (ranging from 1 star to 5) according to their ranking would enhance clarity.

2. Justification of KMPI Parts and Lift Rate: There is a need for clearer justification of KMPI parts, particularly regarding the lift rate as an indicator of model performance. It would be beneficial to explain how the lift rate serves as a relevant metric for evaluating model performance. Furthermore, providing insights into the calculation methodology of T12 of the top decile and whether all top FAs are recruited via this model would offer valuable context. Additionally, offering an illustrative example demonstrating how lift rates serve as indicators for model performance would aid in understanding.

3. Documentation Gaps in Model Inputs: The documentation lacks comprehensive details regarding model inputs. Specifically, there is a need to elaborate on the feature engineering process for categorical data, scaling techniques, handling of outliers and missing values. Moreover, clarifying the relevance of the external discovery data source and elucidating how it complements the internal FA dataset would provide valuable insights. Establishing connections between the internal FA data and external data sources, such as identifying significant business factors from the external data source that influence model output scores, would enhance understanding.

4. Documentation Gaps in Model Methodology: The methodology documentation presents several gaps, including the absence of tests on variable selection, missing hyperparameters, and explanation on the choice of methodology and explanatory variables selection from potential drivers available in publicly available data. Additionally, there is a lack of details regarding out-of-sample testing methodology, potential comparisons between in-sample and out-of-sample testing AUC, and indications of overfitting.

5. Documentation Gaps in Model Use: Clarification is needed regarding the final model output. It is essential to confirm whether the model output solely comprises star ratings or if there are additional outputs generated by the model and presented in the dashboard.

6. Documentation Gaps in Model Implementation: The documentation lacks clarity regarding the examination of the model, responsible parties for conducting the examination, and criteria for determining satisfactory results. Providing details on how the model is examined, by whom, and the parameters for determining satisfactory results would enhance transparency and understanding.
